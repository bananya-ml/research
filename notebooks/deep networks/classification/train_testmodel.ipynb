{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../') # Add the root directory to sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from scipy.signal import hilbert\n",
    "from models.testmodel import CNNWithAttention\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../../../data/Gaia DR3/train.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet(data_dir)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.drop(columns = ['teff_gspphot', 'logg_gspphot', 'mh_gspphot', 'spectraltype_esphs'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing the flux helps our CNN-based model generalize more quickly, but contextual information of the spectrum is lost. We create 2 columns 'min' and 'max' as the minimum and maximum flux density of the spectrum respectively so the model can also learn about the 'position' of our spectra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['min'] = df['flux'].apply(np.min)\n",
    "df['max'] = df['flux'].apply(np.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rerun this cell to see random examples of different spectra\n",
    "\n",
    "\n",
    "# Random sample from 'M' category (massive star)\n",
    "sample_ms = df[df['Cat'] == 'M'].sample(n=1).index\n",
    "flux_ms = df['flux'].iloc[sample_ms].values[0]\n",
    "object_id_ms = df['source_id'].iloc[sample_ms].values[0]\n",
    "\n",
    "# Random sample from 'LM' category (low-mass star)\n",
    "sample_lm = df[df['Cat'] == 'LM'].sample(n=1).index\n",
    "flux_lm = df['flux'].iloc[sample_lm].values[0]\n",
    "object_id_lm = df['source_id'].iloc[sample_lm].values[0]\n",
    "\n",
    "plt.figure(figsize=(20, 6))\n",
    "\n",
    "# Plot for 'M' category (massive star)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(flux_ms)\n",
    "plt.title(f\"Massive Star ({object_id_ms})\")\n",
    "plt.xlabel('Wavelength (nm)')\n",
    "plt.ylabel('Magnitude')\n",
    "\n",
    "# Plot for 'LM' category (low-mass star)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(flux_lm)\n",
    "plt.title(f\"Low-Mass Star ({object_id_lm})\")\n",
    "plt.xlabel('Wavelength (nm)')\n",
    "plt.ylabel('Magnitude')\n",
    "\n",
    "'''\n",
    "flux_ms_h = hilbert(flux_ms)\n",
    "flux_lm_h = hilbert(flux_lm)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(flux_ms_h)\n",
    "plt.title(f\"Massive Star (Hilbert)({object_id_ms})\")\n",
    "plt.xlabel('Wavelength (nm)')\n",
    "plt.ylabel('Magnitude')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(flux_lm_h)\n",
    "plt.title(f\"Low-Mass Star (Hilbert)({object_id_lm})\")\n",
    "plt.xlabel('Wavelength (nm)')\n",
    "plt.ylabel('Magnitude')\n",
    "'''\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = df.shape[0]\n",
    "spectrum_width = len(df['flux'][0])\n",
    "\n",
    "num_samples_lm = df['Cat'].value_counts()['LM']\n",
    "num_samples_m = df['Cat'].value_counts()['M']\n",
    "num_classes = df['Cat'].nunique()\n",
    "\n",
    "print(\"Number of total spectral samples:\", num_samples)\n",
    "print(\"Number of bins in each spectra:\", spectrum_width)\n",
    "print(\"In the dataset, we have\", num_samples_lm, \"spectra for low mass stars and\", num_samples_m, \"spectra for high mass stars.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['flux'].to_numpy()\n",
    "y = df['Cat'].to_numpy()\n",
    "\n",
    "# encode categories\n",
    "y = torch.from_numpy(np.where(y == 'M', 1, np.where(y == 'LM', 0, y)).astype(float))\n",
    "\n",
    "# L2 normalization\n",
    "X = torch.from_numpy(np.array([x / np.linalg.norm(x, keepdims=True) for x in X])).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_curves(curves):\n",
    "    '''\n",
    "    function to extended loss curves to the same length\n",
    "    '''\n",
    "    max_length = max(len(curve) for curve in curves)\n",
    "    \n",
    "    extended_curves = []\n",
    "    for curve in curves:\n",
    "        last_value = curve[-1]\n",
    "        extended_curve = curve + [last_value] * (max_length - len(curve))\n",
    "        extended_curves.append(extended_curve)\n",
    "    \n",
    "    return np.array(extended_curves)\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv1d) or isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            \n",
    "def fit_model(model, x_train, y_train, x_val=None, y_val=None, prt_steps = 1, verbose=True):\n",
    "    \n",
    "    model.apply(init_weights)\n",
    "\n",
    "    # hyperparameters\n",
    "    epochs = 50\n",
    "    learning_rate = 1e-4\n",
    "    batch_size = 64\n",
    "    device = 'cuda'\n",
    "\n",
    "    # early stopping\n",
    "    patience = 10\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    class_weights = torch.tensor(compute_class_weight(class_weight='balanced',classes=np.unique(y_train), y=y_train.numpy())).to(device)\n",
    "\n",
    "    # model components\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights[1])\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "    \n",
    "    # move everything to gpu\n",
    "    model.to(device)\n",
    "    x_train = x_train.to(device)\n",
    "    y_train = y_train.unsqueeze(1).to(device)\n",
    "    x_val = x_val.to(device)\n",
    "    y_val = y_val.unsqueeze(1).to(device)\n",
    "\n",
    "    # metrics\n",
    "    training_losses, validation_losses = [], []\n",
    "    accuracy = []\n",
    "    \n",
    "    # lr cycling\n",
    "    max_lr = 1e-3\n",
    "    steps_per_epoch = (len(x_train) + batch_size - 1) // batch_size\n",
    "    scheduler = OneCycleLR(optimizer, max_lr=max_lr, steps_per_epoch=steps_per_epoch, epochs=epochs)\n",
    "\n",
    "    batch_start = torch.arange(0, len(x_train), batch_size)\n",
    "\n",
    "    t = tqdm(range(epochs), dynamic_ncols=True)\n",
    "\n",
    "    for epoch in t:\n",
    "        t.set_description(f'Epoch {epoch+1}')\n",
    "        \n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "\n",
    "        for start in batch_start:\n",
    "            \n",
    "            x_batch = x_train[start:start+batch_size]\n",
    "            y_batch = y_train[start:start+batch_size]\n",
    "\n",
    "            output = model(x_batch.unsqueeze(1))\n",
    "            loss = criterion(output, y_batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            running_loss += loss.item() * x_batch.size(0)\n",
    "\n",
    "        train_loss = running_loss / len(x_train)\n",
    "        training_losses.append(train_loss)\n",
    "        if verbose and (epoch+1) % prt_steps == 0:\n",
    "            print(f'Train loss: {train_loss:.4f}', end='\\r')\n",
    "\n",
    "\n",
    "        if x_val is not None and y_val is not None:\n",
    "            model.eval()\n",
    "            preds, labels = [], []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                \n",
    "                output = model(x_val.unsqueeze(1))\n",
    "                loss = criterion(output, y_val)\n",
    "\n",
    "                probs = torch.sigmoid(output)\n",
    "                pred = torch.round(probs).cpu().numpy().astype(float)  # pred: [batch_size]\n",
    "                \n",
    "                preds.extend(pred)\n",
    "                labels.extend(y_val.cpu().numpy())\n",
    "                \n",
    "                val_loss = loss.item()\n",
    "            \n",
    "            epoch_acc = accuracy_score(labels, preds)\n",
    "            validation_losses.append(val_loss)\n",
    "            accuracy.append(epoch_acc)\n",
    "            if verbose and (epoch+1) % prt_steps == 0:\n",
    "                print(f'Train loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Accuracy: {epoch_acc:.4f}', end='\\r')\n",
    "\n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                print(f'\\nEarly stopping at epoch {epoch+1}')\n",
    "                break\n",
    "\n",
    "    return model, training_losses, validation_losses, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=10)\n",
    "\n",
    "training_losses_foldx, validation_losses_foldx, accuracy_scores_foldx, models = [], [], [], []\n",
    "\n",
    "model = CNNWithAttention()\n",
    "summary(model)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(X, y)):\n",
    "    \n",
    "    print(f\"\\nFitting fold {fold+1}\")\n",
    "\n",
    "    model, tr_loss, val_loss, acc = fit_model(model, X[train_idx], y[train_idx], X[val_idx], y[val_idx], prt_steps=1, verbose=True)\n",
    "    models.append(model)\n",
    "    training_losses_foldx.append(tr_loss)\n",
    "    validation_losses_foldx.append(val_loss)\n",
    "    accuracy_scores_foldx.append(acc)\n",
    "\n",
    "training_losses = np.mean(extend_curves(training_losses_foldx), axis=0)\n",
    "validation_losses = np.mean(extend_curves(validation_losses_foldx), axis=0)\n",
    "accuracy_scores = np.mean(extend_curves(accuracy_scores_foldx), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 14))\n",
    "\n",
    "ax1.plot(accuracy_scores, label='Validation Accuracy')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_title('Accuracy')\n",
    "ax1.legend(loc='upper left')\n",
    "\n",
    "ax1.annotate(f'{accuracy_scores[0]:.4f}', (0, accuracy_scores[0]), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "ax1.annotate(f'{accuracy_scores[-1]:.4f}', (len(accuracy_scores)-1, accuracy_scores[-1]), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "\n",
    "ax2.plot(training_losses, label='Training Loss')\n",
    "ax2.plot(validation_losses, label='Validation Loss')\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.set_title('Training and Validation Loss')\n",
    "ax2.legend(loc='upper left')\n",
    "\n",
    "ax2.annotate(f'{training_losses[0]:.2f}', (0, training_losses[0]), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "ax2.annotate(f'{training_losses[-1]:.2f}', (len(training_losses)-1, training_losses[-1]), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "ax2.annotate(f'{validation_losses[0]:.2f}', (0, validation_losses[0]), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "ax2.annotate(f'{validation_losses[-1]:.2f}', (len(validation_losses)-1, validation_losses[-1]), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(models,'../../trained_models/test_model_ensemble.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
